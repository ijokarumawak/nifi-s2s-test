
# NIO: Local Macbook to localhost
- 20190201_1244_1.csv: numOfClient=1, numOfTx=10, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=10, totalFailureTx=0, avgCreateTxMillis=58, avgSendMillis=1, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1246_50.csv: numOfClient=50, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=15000, totalFailureTx=0, avgCreateTxMillis=7, avgSendMillis=0, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1254_100.csv: numOfClient=100, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=30000, totalFailureTx=0, avgCreateTxMillis=10, avgSendMillis=0, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1305_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149874, totalFailureTx=126, avgCreateTxMillis=52, avgSendMillis=0, avgConfirmMillis=7, avgCompleteMillis=1
- 20190201_1404_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149825, totalFailureTx=175, avgCreateTxMillis=53, avgSendMillis=0, avgConfirmMillis=5, avgCompleteMillis=1
    - Changed to VolatileProvenanceRepository, but still timeout happens.
    - Next, change InputPort threads from 8 to 16
- 20190201_1411_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149868, totalFailureTx=132, avgCreateTxMillis=54, avgSendMillis=1, avgConfirmMillis=5, avgCompleteMillis=1
    - Still timeout occurs.
- 20190201_1446_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149895, totalFailureTx=105, avgCreateTxMillis=56, avgSendMillis=1, avgConfirmMillis=4, avgCompleteMillis=1
    - After fixing SocketFlowFileServerProtocol's excessive INFO log, number of failures decreased slightly.
    - Next, use another machine for the client.
- 20190201_1532_10.csv: numOfClient=10, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=300, totalFailureTx=0, avgCreateTxMillis=138, avgSendMillis=1, avgConfirmMillis=123, avgCompleteMillis=28
    - Using different machine. But the network is freaky..
        ```
        # ping result
        68 packets transmitted, 67 packets received, 1.5% packet loss
        round-trip min/avg/max/stddev = 2.669/183.117/629.480/215.699 ms
        ```
- 20190201_1543_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=2877, totalFailureTx=123, avgCreateTxMillis=289, avgSendMillis=0, avgConfirmMillis=24, avgCompleteMillis=8

# NIO: AWS EC2 instances
- 20190201_0812_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=3000, totalFailureTx=0, avgCreateTxMillis=607, avgSendMillis=0, avgConfirmMillis=172, avgCompleteMillis=2
    - Using EC2 m5d.large instances. One for standalone NiFi and the other is this client test program.
- 20190201_0816_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=3000, totalFailureTx=0, avgCreateTxMillis=638, avgSendMillis=0, avgConfirmMillis=43, avgCompleteMillis=1
- 20190201_0818_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149295, totalFailureTx=705, avgCreateTxMillis=84, avgSendMillis=0, avgConfirmMillis=79, avgCompleteMillis=2
    - Lots of timeout happens with 500 clients. Does the server encounter any issue? Let's run it again.
- 20190201_0829_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=146506, totalFailureTx=3494, avgCreateTxMillis=97, avgSendMillis=0, avgConfirmMillis=114, avgCompleteMillis=2
    - Probably 500 clients are too much. Let's lower it to 300.
- 20190201_0840_300.csv: numOfClient=300, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=89625, totalFailureTx=375, avgCreateTxMillis=65, avgSendMillis=0, avgConfirmMillis=58, avgCompleteMillis=2
    - Still having timeouts. Let's try 200.
- 20190201_0851_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - It seems having more connection errors at the beginning. Probably both connection maintenance and create transaction try connecting at the same time. Added jitter.
- 20190201_0901_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=59995, totalFailureTx=5, avgCreateTxMillis=31, avgSendMillis=0, avgConfirmMillis=52, avgCompleteMillis=1
    - 200 client seems the maximum concurrency with this environment.

# SOCKET: with NiFi 1.8.0 on EC2
- 20190201_0923_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=60000, totalFailureTx=0, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=44, avgCompleteMillis=1
    - The current implementation works just fine with 200 clients..
- 20190201_0931_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - UpdateAttribute caused backpressure S2S receiving rate exceeds UpdateAttribute.
    - For next: Increased UpdateAttribute batch duration to 1s.
- 20190201_0936_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, - 20190201_0937_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=107304, totalFailureTx=42696, avgCreateTxMillis=46, avgSendMillis=0, avgConfirmMillis=341, avgCompleteMillis=21
    - UpdateAttribute still causes backpressure.
    - [Site-to-Site Listener] o.a.nifi.remote.SocketRemoteSiteListener RemoteSiteListener Unable to accept connection due to java.io.IOException: Too many open files
    - For next: Increased UpdateAttribute concurrent tasks to 4 and max open files from 1024 to 10240
- 20190201_1058_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - Didn't finish.

# Thread/Conn: with JDK11 on EC2
- 20190204_0551_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=60000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=43, avgCompleteMillis=1
- 20190204_0558_400.csv: numOfClient=400, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=120000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=43, avgCompleteMillis=1
- 20190204_0614_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=150000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=44, avgCompleteMillis=1
- 20190204_0622_600.csv: numOfClient=600, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=180000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=44, avgCompleteMillis=1
- 20190204_0630_700.csv: numOfClient=700, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=210000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=46, avgCompleteMillis=1
- 20190204_0639_800.csv: numOfClient=800, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=240000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=46, avgCompleteMillis=2
- 20190204_0647_900.csv: numOfClient=900, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=270000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=47, avgCompleteMillis=2
- 20190204_0655_1000.csv: numOfClient=1000, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=300000, totalFailureTx=0, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=49, avgCompleteMillis=2
    - Succeeded test with 1000 clients.
- 20190204_0704_1200.csv: numOfClient=1200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=358204, totalFailureTx=1796, avgCreateTxMillis=17, avgSendMillis=0, avgConfirmMillis=105, avgCompleteMillis=4
    - Server listener thread stops working due to OOM
- 20190204_0759_1100.csv: numOfClient=1100, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=329976, totalFailureTx=24, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=64, avgCompleteMillis=2
    - After restarting NiFi, 1100 clients worked but some failure occurred.

# NIO from scratch
These tests used plain RAW connection (not secured) since secure connection hasn't implemented yet.
Just to confirm multiple clients work fine with a reasonable performance.
- 20190423_1406_10.csv: numOfClient=10, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=3000, totalFailureTx=0, avgCreateTxMillis=5, avgSendMillis=0, avgConfirmMillis=6, avgCompleteMillis=0
- 20190423_1424_100.csv: numOfClient=100, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=30000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=5, avgCompleteMillis=0
- 20190423_1432_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=60000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=6, avgCompleteMillis=0
- 20190423_1443_300.csv: numOfClient=300, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=90000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=6, avgCompleteMillis=1
- 20190423_1453_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=150000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=11, avgCompleteMillis=3
- 20190423_1501_700.csv: numOfClient=700, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=210000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=164, avgCompleteMillis=7
- 20190423_1511_800.csv: numOfClient=800, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=240000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=1115, avgCompleteMillis=12
- 20190423_1539_1000.csv: numOfClient=1000, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, - 20190423_1542_1000.csv: numOfClient=1000, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=300000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=2009, avgCompleteMillis=16
- Increased input port concurrency from 4 to 6
- 20190423_1602_1200.csv: numOfClient=1200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=359258, totalFailureTx=742, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=2660, avgCompleteMillis=25
- 20190423_1622_1200.csv: numOfClient=1200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=359043, totalFailureTx=957, avgCreateTxMillis=3, avgSendMillis=0, avgConfirmMillis=3082, avgCompleteMillis=28

2019-04-23 16:40:33,568 ERROR [NIO-Site-to-Site Listener] o.a.n.remote.nio.MessageSequenceHandler Failed to process mess
age due to org.apache.nifi.processor.exception.ProcessException: org.apache.nifi.remote.exception.RequestExpiredException
org.apache.nifi.processor.exception.ProcessException: org.apache.nifi.remote.exception.RequestExpiredException
        at org.apache.nifi.remote.StandardRootGroupPort.startReceivingFlowFiles(StandardRootGroupPort.java:443)
        at org.apache.nifi.remote.nio.s2s.ReceiveFlowFiles.lambda$new$0(ReceiveFlowFiles.java:53)
        at org.apache.nifi.remote.nio.stream.StreamData.prepare(StreamData.java:42)
        at org.apache.nifi.remote.nio.MessageSequenceHandler.processBuffers(MessageSequenceHandler.java:149)
        at org.apache.nifi.remote.nio.MessageSequenceHandler.selectKeys(MessageSequenceHandler.java:230)
        at org.apache.nifi.remote.nio.MessageSequenceHandler.start(MessageSequenceHandler.java:57)
        at org.apache.nifi.remote.nio.s2s.NIORemoteSiteListener.lambda$start$3(NIORemoteSiteListener.java:79)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.nifi.remote.exception.RequestExpiredException: null
        at org.apache.nifi.remote.StandardRootGroupPort.startReceivingFlowFiles(StandardRootGroupPort.java:434)
        ... 7 common frames omitted

# Profiling
- 20190424_0935_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=59999, totalFailureTx=1, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=14, avgCompleteMillis=1
- 20190424_0942_200.csv: numOfClient=200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=100000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=13, avgCompleteMillis=2
- Changed input port concurrency form 6 to 16.
- 20190424_0953_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=499864, totalFailureTx=136, avgCreateTxMillis=11, avgSendMillis=0, avgConfirmMillis=1119, avgCompleteMillis=44
- Change Provenance Repo to volatile
- 20190424_1033_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=499957, totalFailureTx=43, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=558, avgCompleteMillis=32
- 20190424_1054_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=499941, totalFailureTx=59, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=630, avgCompleteMillis=33

Does the server closes connections?
10:55:42.892 [Thread-913] ERROR com.rumawaks.nifi.s2s.test.S2SClient - Transaction failed due to java.net.SocketExceptio
n: Connection reset by peer
10:55:44.898 [Thread-913] ERROR com.rumawaks.nifi.s2s.test.S2SClient - Transaction failed due to java.lang.NullPointerEx
ception
10:55:46.150 [Thread-913] ERROR com.rumawaks.nifi.s2s.test.S2SClient - Transaction failed due to java.net.SocketException: Connection reset by peer
- 20190424_1128_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=493946, totalFailureTx=6054, avgCreateTxMillis=4, avgSendMillis=0, avgConfirmMillis=421, avgCompleteMillis=30

# NIO 10 worker threads
- 20190424_1630_10.csv: numOfClient=10, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=0, totalFailureTx=5000, avgCreateTxMillis=-1, avgSendMillis=-1, avgConfirmMillis=-1, avgCompleteMillis=-1
- 20190424_1705_10.csv: numOfClient=10, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=5000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=5, avgCompleteMillis=0
- 20190424_1719_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=500000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=55, avgCompleteMillis=15
- 20190424_1731_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, - 20190425_0832_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=597886, totalFailureTx=2114, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=1649, avgCompleteMillis=50
- The cause of failure was that StandardRootGroupPort failed to put new FlowFileRequest into its requestQueue which has size limitation as 1,000. That means it can support only 1000 concurrent requests.
```
2019-04-25 08:50:49,883 ERROR [NIOSiteToSite-IO-2] o.a.n.remote.nio.MessageSequenceHandler Failed to process message due to org.apache.nifi.processor.exception.ProcessException: org.apache.nifi.remote.exception.RequestExpiredException
org.apache.nifi.processor.exception.ProcessException: org.apache.nifi.remote.exception.RequestExpiredException
        at org.apache.nifi.remote.StandardRootGroupPort.startReceivingFlowFiles(StandardRootGroupPort.java:443)
        at org.apache.nifi.remote.nio.s2s.ReceiveFlowFiles.lambda$new$0(ReceiveFlowFiles.java:53)
```

## Increased requestQueue size to 10,000 (on a single Macbook)
- 20190425_0937_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=583900, totalFailureTx=16100, avgCreateTxMillis=5, avgSendMillis=0, avgConfirmMillis=1705, avgCompleteMillis=50
- 20190425_1005_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000,
- 20190425_1043_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=564386, totalFailureTx=35614, avgCreateTxMillis=6, avgSendMillis=0, avgConfirmMillis=1406, avgCompleteMillis=47

  10 java.io.IOException:Brokenpipe
   8 java.io.IOException:Connectionreset
   1 java.io.IOException:Protocolwrong
  52 java.net.ConnectException:Operationtimed
  19 java.net.SocketException:Connectionreset

## Implemented secure connection (on two AWS instances)

- 20190514_0356_100.csv: numOfClient=100, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=50000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=47, avgCompleteMillis=1
- 20190514_0407_1000.csv: numOfClient=1000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=500000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=145, avgCompleteMillis=14
- 20190514_0425_1100.csv: numOfClient=1100, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=550000, totalFailureTx=0, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=204, avgCompleteMillis=17
- 20190514_0439_1200.csv: numOfClient=1200, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=600000, totalFailureTx=0, avgCreateTxMillis=10, avgSendMillis=1, avgConfirmMillis=660, avgCompleteMillis=26
- 20190514_0457_1500.csv: numOfClient=1500, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, 

The client machine's load-average went up more than 600. It seems 1000 concurrent client may be the hard limit for this VM spec.

## Changed the client EC2 from m5.large to m5.xlarge

- 20190514_0526_1500.csv: numOfClient=1500, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=750000, totalFailureTx=0, avgCreateTxMillis=7, avgSendMillis=0, avgConfirmMillis=1306, avgCompleteMillis=28

## Changed the client EC2 from m5.xlarge to m5.4xlarge

- 20190514_0717_1500.csv: numOfClient=1500, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=742985, totalFailureTx=7015, avgCreateTxMillis=3, avgSendMillis=0, avgConfirmMillis=1243, avgCompleteMillis=28

Server side load average was around 6 and 20 on the client machine.
The errors were connection timeouts.
Seems like the clients sends requests more actively and reached to the max server capability.

## Changed the server EC2 from m5.large to m5.xlarge

- 20190514_0800_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=998491, totalFailureTx=1509, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=1103, avgCompleteMillis=20
- How about the socket version?
- 20190514_0837_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=927934, totalFailureTx=72066, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=48, avgCompleteMillis=1

## NIO Increased IO thread pool from 10 to 100

- 20190515_0830_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=989982, totalFailureTx=10018, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=1415, avgCompleteMillis=23

Increasing the number of IO thread didn't help.

## With site-to-site-client 1.10.0-SNAPSHOT

The latest S2S client adopts a lock to prevent multiple threads fetches peer list at the same time for a client instance. It should help reducing the contention around fetch peer list.

- 20190516_0057_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1000000, totalFailureTx=0, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=1612, avgCompleteMillis=25

Profiled NiFi server process using Yourkit at the same time. There was no obvious bottle-neck.
Added more logs to get more insights.

- 20190516_0143_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1000000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=949, avgCompleteMillis=19

Changed InputPort concurrent tasks from 16 to 32

- 20190516_0220_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=893437, totalFailureTx=106563, avgCreateTxMillis=24, avgSendMillis=0, avgConfirmMillis=686, avgCompleteMillis=35

To reduce latency, the InputPort's concurrency should be increased.
But it produces more NW error, probably due to NW contention.

InputPort concurrent tasks = 128
IO threads = 20

- 20190516_0245_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=975559, totalFailureTx=24441, avgCreateTxMillis=12, avgSendMillis=0, avgConfirmMillis=285, avgCompleteMillis=45

## To accept more connections

InputPort concurrent tasks = 20
IO threads = 20
- 20190516_0304_2000.csv: numOfClient=2000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1000000, totalFailureTx=0, avgCreateTxMillis=1, avgSendMillis=0, avgConfirmMillis=861, avgCompleteMillis=21

How about 2500 clients?
Didn't run successfully. Lots of timeout exception happend. Server's CPU usage was over 80%.

4 CPU cores and 2000 clients may be a guide line.

## 8 CPU cores?

InputPort concurrent tasks = 40
IO threads = 40
- 20190516_0447_2500.csv: numOfClient=2500, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1250000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=118, avgCompleteMillis=14
- 20190516_0505_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1444367, totalFailureTx=55633, avgCreateTxMillis=9, avgSendMillis=0, avgConfirmMillis=149, avgCompleteMillis=17

InputPort concurrent tasks = 30
IO threads = 30
- 20190516_0544_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1461914, totalFailureTx=38086, avgCreateTxMillis=6, avgSendMillis=0, avgConfirmMillis=212, avgCompleteMillis=15

InputPort concurrent tasks = 20
IO threads = 20

- 20190516_0609_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1465431, totalFailureTx=34569, avgCreateTxMillis=6, avgSendMillis=0, avgConfirmMillis=541, avgCompleteMillis=15
- 20190516_0631_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=10, totalSuccessfulTx=1500000, totalFailureTx=0, avgCreateTxMillis=0, avgSendMillis=0, avgConfirmMillis=57, avgCompleteMillis=2

With short interval, the number of concurrent task can be less because each client may finish all transactions before others start.


- 20190516_0648_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, - 20190516_0710_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, 

SSLPeerUnverifiedException occurs with JDK 11.0.2.
Caused by: java.security.cert.CertificateException: javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated
https://bugs.openjdk.java.net/browse/JDK-8220723

Try using cached DN.
- 20190516_0746_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1471405, totalFailureTx=28595, avgCreateTxMillis=6, avgSendMillis=0, avgConfirmMillis=566, avgCompleteMillis=15

Now all errors are timeout. Let's increase threads.
InputPort concurrent tasks = 40
IO threads = 40

- 20190516_0813_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1465357, totalFailureTx=34643, avgCreateTxMillis=7, avgSendMillis=0, avgConfirmMillis=139, avgCompleteMillis=16

InputPort concurrent tasks = 100
IO threads = 40
- 20190516_0837_3000.csv: numOfClient=3000, numOfTx=500, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=1447821, totalFailureTx=52179, avgCreateTxMillis=6, avgSendMillis=0, avgConfirmMillis=130, avgCompleteMillis=19
