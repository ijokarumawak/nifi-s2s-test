
# Local Macbook to localhost
- 20190201_1244_1.csv: numOfClient=1, numOfTx=10, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=10, totalFailureTx=0, avgCreateTxMillis=58, avgSendMillis=1, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1246_50.csv: numOfClient=50, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=15000, totalFailureTx=0, avgCreateTxMillis=7, avgSendMillis=0, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1254_100.csv: numOfClient=100, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=30000, totalFailureTx=0, avgCreateTxMillis=10, avgSendMillis=0, avgConfirmMillis=1, avgCompleteMillis=1
- 20190201_1305_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149874, totalFailureTx=126, avgCreateTxMillis=52, avgSendMillis=0, avgConfirmMillis=7, avgCompleteMillis=1
- 20190201_1404_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149825, totalFailureTx=175, avgCreateTxMillis=53, avgSendMillis=0, avgConfirmMillis=5, avgCompleteMillis=1
    - Changed to VolatileProvenanceRepository, but still timeout happens.
    - Next, change InputPort threads from 8 to 16
- 20190201_1411_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149868, totalFailureTx=132, avgCreateTxMillis=54, avgSendMillis=1, avgConfirmMillis=5, avgCompleteMillis=1
    - Still timeout occurs.
- 20190201_1446_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149895, totalFailureTx=105, avgCreateTxMillis=56, avgSendMillis=1, avgConfirmMillis=4, avgCompleteMillis=1
    - After fixing SocketFlowFileServerProtocol's excessive INFO log, number of failures decreased slightly.
    - Next, use another machine for the client.
- 20190201_1532_10.csv: numOfClient=10, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=300, totalFailureTx=0, avgCreateTxMillis=138, avgSendMillis=1, avgConfirmMillis=123, avgCompleteMillis=28
    - Using different machine. But the network is freaky..
        ```
        # ping result
        68 packets transmitted, 67 packets received, 1.5% packet loss
        round-trip min/avg/max/stddev = 2.669/183.117/629.480/215.699 ms
        ```
- 20190201_1543_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=2877, totalFailureTx=123, avgCreateTxMillis=289, avgSendMillis=0, avgConfirmMillis=24, avgCompleteMillis=8

# AWS EC2 instances
- 20190201_0812_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=3000, totalFailureTx=0, avgCreateTxMillis=607, avgSendMillis=0, avgConfirmMillis=172, avgCompleteMillis=2
    - Using EC2 m5d.large instances. One for standalone NiFi and the other is this client test program.
- 20190201_0816_100.csv: numOfClient=100, numOfTx=30, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=3000, totalFailureTx=0, avgCreateTxMillis=638, avgSendMillis=0, avgConfirmMillis=43, avgCompleteMillis=1
- 20190201_0818_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=149295, totalFailureTx=705, avgCreateTxMillis=84, avgSendMillis=0, avgConfirmMillis=79, avgCompleteMillis=2
    - Lots of timeout happens with 500 clients. Does the server encounter any issue? Let's run it again.
- 20190201_0829_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=146506, totalFailureTx=3494, avgCreateTxMillis=97, avgSendMillis=0, avgConfirmMillis=114, avgCompleteMillis=2
    - Probably 500 clients are too much. Let's lower it to 300.
- 20190201_0840_300.csv: numOfClient=300, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=89625, totalFailureTx=375, avgCreateTxMillis=65, avgSendMillis=0, avgConfirmMillis=58, avgCompleteMillis=2
    - Still having timeouts. Let's try 200.
- 20190201_0851_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - It seems having more connection errors at the beginning. Probably both connection maintenance and create transaction try connecting at the same time. Added jitter.
- 20190201_0901_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=59995, totalFailureTx=5, avgCreateTxMillis=31, avgSendMillis=0, avgConfirmMillis=52, avgCompleteMillis=1
    - 200 client seems the maximum concurrency with this environment.

## with NiFi 1.8.0 on EC2
- 20190201_0923_200.csv: numOfClient=200, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=60000, totalFailureTx=0, avgCreateTxMillis=2, avgSendMillis=0, avgConfirmMillis=44, avgCompleteMillis=1
    - The current implementation works just fine with 200 clients..
- 20190201_0931_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - UpdateAttribute caused backpressure S2S receiving rate exceeds UpdateAttribute.
    - For next: Increased UpdateAttribute batch duration to 1s.
- 20190201_0936_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, - 20190201_0937_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, totalSuccessfulTx=107304, totalFailureTx=42696, avgCreateTxMillis=46, avgSendMillis=0, avgConfirmMillis=341, avgCompleteMillis=21
    - UpdateAttribute still causes backpressure.
    - [Site-to-Site Listener] o.a.nifi.remote.SocketRemoteSiteListener RemoteSiteListener Unable to accept connection due to java.io.IOException: Too many open files
    - For next: Increased UpdateAttribute concurrent tasks to 4 and max open files from 1024 to 10240
- 20190201_1058_500.csv: numOfClient=500, numOfTx=300, numOfPacketsPerTx=5, txIntervalMillis=1000, 
    - Didn't finish.
